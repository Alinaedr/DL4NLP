{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chapter 1: Deep Learning for Natural Language Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 A Selection of Machine Learning Methods for NLP\n",
    "\n",
    "**Goal of classification algorithms:**\n",
    "    - Linearly separate data according to _classes_.\n",
    "\n",
    "**Classes:**\n",
    "- Labels indicating a (usually exclusive) category that a data point belongs to.\n",
    "\n",
    "**Input Space**\n",
    "- The vector representations of data presented to machine learning algorithms.\n",
    "\n",
    "**Feature Space**\n",
    "- The processing, manipulation and abstraction of the input space during learning.\n",
    "- can be done externally (i.e. pre-processing: converting raw data to features)\n",
    "\n",
    "\n",
    "**Output Space**\n",
    "- class labels that separate data points based on class boundaries\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1.1 The Perception\n",
    "\n",
    "**Given:** A vector of features describing aspects of something (eg, words in a document)\n",
    "**Goal:** To create a function that maps from features to binary label.\n",
    "$$F(\\text{feature vector}) \\rightarrow (0 | 1)$$\n",
    "\n",
    "The single-layer perceptron does this from a weighted combination of input values, $x_1, \\dots , x_n$, based on a threshold $\\theta$ and a bias, $b$\n",
    "\n",
    "**Perceptron Decision Function**\n",
    "The weights, $w_1, w_2, ...,w_n $ are learned from _annotated_ training data (input vectors labeled with output labels).\n",
    "**Neuron**:\n",
    " - The threshold unit which receives the summed and weighted inputs $v$.\n",
    "$$ input space=[10, 20, 30]$$\n",
    "$$ weights = [3, 5, 7]$$\n",
    "$$\\text{feature space} = (10 * 3) + (20 * 5) + (30 * 7) $$\n",
    "$$\\text{neuron output} = 340$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.linear_model.perceptron module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Religion is widespread, even in modern times' => alt.atheism\n",
      "'His kidney failed and he died' => sci.med\n",
      "'The Pope is a controversial leader of the Catholic church' => sci.med\n",
      "'White blood cells fight off infections' => sci.med\n",
      "'The reverend had a heart attack in church' => sci.med\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import perceptron\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Make a subselection for two newsgroups of interest\n",
    "categories = ['alt.atheism', 'sci.med']\n",
    "\n",
    "# Train a simple perceptron on a vector representation of the documents in these two classes.\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True)\n",
    "\n",
    "perceptron = perceptron.Perceptron(max_iter=100)\n",
    "# Make the Vector representations\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_train_counts = cv.fit_transform(train.data) # tra\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_tf = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_tf.fit_transform(X_train_counts) # Turns documents into vectors that can be def into ML algorithm\n",
    "\n",
    "perceptron.fit(X_train_tfidf, train.target)\n",
    "\n",
    "test_docs = ['Religion is widespread, even in modern times',\n",
    "             'His kidney failed and he died',\n",
    "             'The Pope is a controversial leader of the Catholic church',\n",
    "             'White blood cells fight off infections',\n",
    "             'The reverend had a heart attack in church']\n",
    "\n",
    "X_test_counts = cv.transform(test_docs) # count vectors of test data\n",
    "X_test_tfidf = tfidf_tf.transform(X_test_counts) # TF_IDF vectors of test data\n",
    "\n",
    "pred = perceptron.predict(X_test_tfidf)\n",
    "\n",
    "for doc, category in zip(test_docs, pred):\n",
    "    print('%r => %s' % (doc, train.target_names[category]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1.2 Support Vector Machines\n",
    "A binary classifier that implicitly maps data in feature space to higher dimensions in which data becomes separable by a linear _hyperplane_. This mapping is carried out by a kernal function.\n",
    "\n",
    "**Kernel Functions**:\n",
    "- transforms the original input space to an alternative representation that implicitly has a higher dimensionality.\n",
    "- The migration from lower to higher dimensionality space in takes the form of a similarity function applied to two feature vectors\n",
    "- Kernals functions take two vectors, mixes in a constant (a kernel parameter) and adds some kern-specific ingredients to produce a specific form of a dot product of the two vectors.\n",
    "\n",
    "e.g. Quadratic Polynomial Kernel - using a constant, $c$, and inputs $x=(x_1, x_2)$, and $y=(y_1, y_2)$:\n",
    "$ K(x, y) = (c \\plus x^Ty)^2  $\n",
    "$ K(x, y)  = (c + x_1y_1 \\plus x_2y_2)^2$\n",
    "$ K(x, y) = c^2 \\plus x_{1}^2y_{1}^2 \\plus x_{2}^2x_{2}^2 \\plus 2cx_{1}y_{1} \\plus 2cx_{2}y_{2} \\plus 2x_1y_1x_2y_2$\n",
    "\n",
    "i.e. we go from a 2 to a 6 dimensional (factors separated by +) space. So we're _implicitly_ computing the dot product of two vector.\n",
    "$$< c, x_{1}^2, x_{2}^2, \\sqrt{2cx_1}, \\sqrt{2cx_2}, \\sqrt{2cx_{1}x_2}>$$\n",
    "$$< c, y_{1}^2, y_{2}^2, \\sqrt{2cy_1}, \\sqrt{2cy_2}, \\sqrt{2cy_{1}y_2}>$$\n",
    "\n",
    "In the transformed space, the two classes are separated with maximally wide boundaries. The datapoints determining the slope of these boundaries are called **support vectors**.\n",
    "\n",
    "During training, SVMs learn weights that optimize the margins with the least error.\n",
    "At test, new inputs are projected onto the support vectors, and depending on which side it lands, it recieves a positive or negative label.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "1073.0587390970015"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = 3\n",
    "x = [2, 5]\n",
    "y = [3,6]\n",
    "k = (c + np.transpose(x)* y)**2\n",
    "x_ = [c, x[0]**2, x[1]**2, np.sqrt(2*c*x[0]), np.sqrt(2*c*x[1]), np.sqrt(2*c*x[0]*x[1])]\n",
    "y_ = [c, y[0]**2, y[1]**2, np.sqrt(2*c*y[0]), np.sqrt(2*c*y[1]), np.sqrt(2*c*y[0]*y[1])]\n",
    "np.dot(x_, y_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1.3 Memory-based Learning\n",
    "\n",
    "* Rather than replace training data with support vectors,memory-based methods keep all training data.\n",
    "* During classification (test) input data is matched with training data by application of similarity measures.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def delta(x,y):\n",
    "    if x == y:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def IB1(a, b):\n",
    "    return sum([delta(a_i, b_i) for a_i, b_i in zip(a,b)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This metric computes the distance between two feature vectors on the basis of **feature value overlap**.\n",
    "Most memory-based learning algorithms are extend these distance metrics with feature-weighting.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Deep Learning\n",
    "Which questions can deep learning solve for natural language processing?\n",
    "* Deep learning can handle a large number of parameters, each of which encode some aspect of input data.\n",
    "\n",
    "Learns hierarchical representations of data. (lower levels feed into higher ones)\n",
    "- Layers = complex functions processing inputs with weights that encode the importance of information for labeling purposes.\n",
    "- Output layers = produce a label\n",
    "- Hidden layers = layers in between input and output layers. heirarchical from specific (close to the input layer) to abstract (closer to the output layer).\n",
    "    - it's hard to come up with human-understandable interpretations of hidden layer representations.\n",
    "\n",
    "Training:Estimating weights in the whole work of neural networks.\n",
    "\n",
    "Backprop- involves the stepwise minimization of error (gradient descent)\n",
    "- The goal of which is to tune network weights, until the slope of the error function (predicted minus observed) is close to zero.\n",
    "    - Can use the chain rule from calculus to compute the slope of functions applied to other functions.  f(g(x)).\n",
    "  $$\\text{activation function}_{\\text{output layer}_N}( \\text{activation function}_{\\text{hidden layer}_{N-1}}(x))$$\n",
    "\n",
    "  Where the hidden layer, $N-1$ is just before the output layer and the $x$ refers to the outputs of the hidden layer before $N-2$. Meaning weight adaptations never reach hidden layers closer to the input layer.\n",
    "**solution: Restricted Boltzman Machines** - basically a complete neural network with backprop at every layer.\n",
    "\n",
    "**Rectified Linear Unit (ReLU) Activation function**\n",
    "Determines if the input to a neuron is above some threshold in order to \"activate\" it and propagate information to the next layer.\n",
    "$$y = ReLU(0, \\sum{(input_i * weight+_i)+ bias})$$\n",
    "Will return the maximum of the two values.\n",
    "Derivative of ReLU function :\n",
    "$$ReLU'(x) = \\begin{cases}\n",
    "            1, &\\text{if}\\ x > 0 \\\\\n",
    "             0, & \\text{else}\n",
    "             \\end{cases}$$\n",
    "\n",
    "**Sigmoid Activation function**\n",
    "$$sigmoid(x) = \\frac{1}{1 \\plus e^{-x}}$$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-layer Perceptron\n",
    "**Scenario**: You want to train a deep network on a sentiment labeling task. The task consists of labeling texts with sentiment labels: 1 for positive sentiment, and 0 for negative. You are unsure about which activation function you should choose. Can you find out experimentally the best option?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"\\ndfolders = glob(os.path.join('review_polarity/txt_sentoken', '*'))\\nxx = pd.concat([pd.read_csv(file, sep='\\t', header=None) for file in glob(os.path.join(dfolders[0], '*.txt'))])\\nxx.columns=['text']\\nxx['label'] = 0\\n\\nyy = pd.concat([pd.read_csv(file, sep='\\t', header=None) for file in glob(os.path.join(dfolders[1], '*.txt'))])\\nyy.columns = ['text']\\nyy['label'] = 1\\ndata  = pd.concat([xx,yy])\\ndocs = data['text']\\n\\ntokenizer = Tokenizer()\\ntokenizer.fit_on_texts(docs)\\n\""
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import  Tokenizer\n",
    "from keras.layers.core import Dense, Activation\n",
    "import pandas as pd\n",
    "import sys\n",
    "from glob import glob\n",
    "import os\n",
    "'''\n",
    "dfolders = glob(os.path.join('review_polarity/txt_sentoken', '*'))\n",
    "xx = pd.concat([pd.read_csv(file, sep='\\t', header=None) for file in glob(os.path.join(dfolders[0], '*.txt'))])\n",
    "xx.columns=['text']\n",
    "xx['label'] = 0\n",
    "\n",
    "yy = pd.concat([pd.read_csv(file, sep='\\t', header=None) for file in glob(os.path.join(dfolders[1], '*.txt'))])\n",
    "yy.columns = ['text']\n",
    "yy['label'] = 1\n",
    "data  = pd.concat([xx,yy])\n",
    "docs = data['text']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "a = pd.read_csv(glob(os.path.join('review_polarity/txt_sentoken/neg', '*'))[0], sep='\\t', header=0)\n",
    "a.columns = ['text']\n",
    "a['label'] = 0\n",
    "b = pd.read_csv(glob(os.path.join('review_polarity/txt_sentoken/pos', '*'))[0], sep='\\t', header=0)\n",
    "b.columns = ['text']\n",
    "b['label'] = 1\n",
    "\n",
    "data = pd.concat([a, b])\n",
    "docs = data['text']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "X_train = tokenizer.texts_to_matrix(docs, mode='binary')\n",
    "Y_train = np_utils.to_categorical(data['label'])\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "nb_classes = Y_train.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 95 samples, validate on 11 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 1.0353 - accuracy: 0.3368 - val_loss: 0.6842 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.7592 - accuracy: 0.3263 - val_loss: 1.0705 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6969 - accuracy: 0.6632 - val_loss: 1.3206 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6842 - accuracy: 0.6632 - val_loss: 1.4261 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6737 - accuracy: 0.6632 - val_loss: 1.4226 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6592 - accuracy: 0.6632 - val_loss: 1.3544 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6448 - accuracy: 0.6632 - val_loss: 1.2581 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6352 - accuracy: 0.6632 - val_loss: 1.1597 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6325 - accuracy: 0.6632 - val_loss: 1.0767 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6352 - accuracy: 0.6632 - val_loss: 1.0168 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7fc2e2ff06d0>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128,input_dim=input_dim)) #128 neurons\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training...')\n",
    "model.fit(x=X_train, y=Y_train, epochs=10, batch_size=32, validation_split=0.1, shuffle=False, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results with sigmoid activation function - getting the Data inputs are different.\n",
    "accuracy does not improve above .66. _now lets try using a ReLU actiation function instead._"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 95 samples, validate on 11 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.7023 - accuracy: 0.3263 - val_loss: 0.7226 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6872 - accuracy: 0.6632 - val_loss: 0.7438 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6774 - accuracy: 0.6632 - val_loss: 0.7797 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6605 - accuracy: 0.6632 - val_loss: 0.8516 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6257 - accuracy: 0.6632 - val_loss: 1.0057 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.5624 - accuracy: 0.6632 - val_loss: 1.3371 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.4742 - accuracy: 0.6632 - val_loss: 1.9082 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.3876 - accuracy: 0.6632 - val_loss: 2.5019 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.3208 - accuracy: 0.6632 - val_loss: 2.8874 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.2819 - accuracy: 0.6632 - val_loss: 3.1514 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7fdba3f9ba10>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128,input_dim=input_dim)) #128 neurons\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training...')\n",
    "model.fit(x=X_train, y=Y_train, epochs=10, batch_size=32, validation_split=0.1, shuffle=False, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Vector Representations of Language\n",
    "Vector: A point in some multi-dimensional space.\n",
    "Machine Learning is all about measuring the distances between points in these spaces.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.1 Representational vectors\n",
    "Describe text across a number of human-interpretable feature dimensions.\n",
    "_Bag-of-words Approach_:\n",
    "Every dimension can be interpreted as representing a clear feature dimension:The presence(binary) of a certain word in an index lexicon.\n",
    "Essentially:\n",
    "* create a list of the unique words in a corpus. - The lexicon\n",
    "* Give each one a unique number identifier\n",
    "Then:\n",
    "* Each sentence in the corpus can be represented as an $N$ length binary vector where $N$ represents the total number of unique words.\n",
    "* The values in the N-vector correspond to the words in the lexicon, whether or not a lexicon word is present in the sentence.\n",
    "\n",
    "These can also be counts (the number of times a word appears in a sentence) rather than a binary variable.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 1 1 0 1 0]\n",
      " [1 1 1 0 0 0 1 1 1 1]]\n",
      "{'natural': 8, 'language': 6, 'is': 5, 'hard': 4, 'for': 3, 'computers': 2, 'are': 0, 'capable': 1, 'of': 9, 'learning': 7}\n"
     ]
    }
   ],
   "source": [
    "train_dat = pd.DataFrame({\n",
    "    \"text\":[\"Natural language is hard for computers\", \"Computers are capable of learning natural language\"],\n",
    "    \"label\": [0, 1]\n",
    "})\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(analyzer=\"word\",\n",
    "                     tokenizer=None,\n",
    "                     preprocessor=None,\n",
    "                     stop_words=None,\n",
    "                     max_features=1000)\n",
    "\n",
    "doc_cv = cv.fit_transform(train_dat['text']).toarray()\n",
    "print(doc_cv)\n",
    "print(cv.vocabulary_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.2 Operational vectors\n",
    "typically Not human-interpretable and vector values are derived from some algorithm.\n",
    "\n",
    "**tf.idf**:\n",
    "* vector values represent word weights computed as $\\text{term frequency} \\times \\text{inverse document frequency}$, which expresses a degree of saliency.\n",
    "* _term frequency_  - number of times a word appears in a document.\n",
    "* _inverse document frequency_ - the frequency of the word in other documents in a collection of documents.\n",
    "\n",
    "**Neural word embeddings**\n",
    "* \"word2vec models\": embeddings from NNs that predict words given context, or context given word.\n",
    "* \"distributional semantic similarity\": vectors that are close in distance\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Vector Sanitization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4.1 The Hashing Trick\n",
    "Because large vectors can take up too many compute resources unnecessarily, we use **feature hashing** to reduce the dimensionality.\n",
    "* a hashing function maps every feature to an index, and the hashing trick algorithm updates the information at those indices only."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "InverseLexicon = {'integer':'word' }\n",
    "hash_function = {'feature': 'index'}\n",
    "def feat_hash(featureV, vecSize):\n",
    "    outputV = np.array(vecSize)\n",
    "    for f in range(len(featureV)):\n",
    "        if featureV[f] == 1:\n",
    "            dim=hash_function[InverseLexicon[f]]\n",
    "            outputV[dim % vecSize] +=1\n",
    "    return outputV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4.2 Vector Normalization\n",
    "* reducing the length of a vector to one, by dividing each elements by the sum of all elements in the vector."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f7c24000",
   "language": "python",
   "display_name": "PyCharm (mTurkBx)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}