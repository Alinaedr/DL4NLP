{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chapter 2: Deep Learning and Language: The Basics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1: Basic Architectures of Deep Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.1: Deep Multilayer Perceptrons\n",
    "\n",
    "artificial  neurons are mathematical functions that receive weighted input from their afferents."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Convolution1D, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. `.fit_on_text` : creates a vocabulary index based on word frequency. mapping a word to it's index within text\n",
    "    * e.g. given `\"the cat sat on the mat\"` , then it will create a dictionary such that `word_index['sat']` will return `2`.\n",
    "    * The most frequent words are earlier indices so, `word_index[\"the\"]` will return `0`\n",
    "    * Often, the first few indices will be stop words because they appear a lot.\n",
    "2. `.text_to_matrix` : creates a numpy matrix with rows corresponding to the number of documents and columns corresponding to the unique words in the vocabulary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "data = pd.read_csv('../raaijmakers-master-code/code/pos_neg.txt', sep='\\t',  encoding = \"ISO-8859-1\")\n",
    "docs = data['text']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "X_train = tokenizer.texts_to_matrix(docs, mode='binary')\n",
    "Y_train = np_utils.to_categorical(data['label'])  # from [1, 1, 0] --> [(0,1), (0, 1), (1, 0)]\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "nb_classes = Y_train.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "856"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.word_index['bad']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Building the model\n",
    "1. The Sequential model serves as a container for the stacked layers.\n",
    "2. Add a dense layer to receive inputs (of dimensionality defined above) and return an output of 128 dimensions\n",
    "3. Add another layer that receives, as inputs, the outputs of the Dense layer in 2 and uses a ReLU activation function to determine its output."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim))\n",
    "model.add(Activation('relu'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding more layers...."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once you are finished adding layers, `compile` by specifying:\n",
    " * the loss(error) function\n",
    " * a numerical optimizer algorithm to carry out the gradient descent process\n",
    " * a metric to evaluate performance.\n",
    "\n",
    "Next, you can fit the model specifying\n",
    " * validation_split: the proportion of data held out of training and used for testing\n",
    " * the number of training epochs\n",
    " * batch size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.6927 - accuracy: 0.5500 - val_loss: 0.7872 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6473 - accuracy: 0.5556 - val_loss: 0.8588 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.5316 - accuracy: 0.6667 - val_loss: 1.0495 - val_accuracy: 0.0500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.2952 - accuracy: 0.9556 - val_loss: 1.1513 - val_accuracy: 0.2000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0693 - accuracy: 1.0000 - val_loss: 0.4489 - val_accuracy: 0.7500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.8451e-04 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.4050e-05 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 2.3749e-06 - accuracy: 1.0000 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 8.3746e-07 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7fa88e1ace50>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.1, shuffle=True,verbose=2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.2: Spatial and Temporal Operators\n",
    "\n",
    "**Spatial Filtering: Convolutional Neural Networks**\n",
    "The goal of spatial filtering is to deal with the _structure_ of input data by filtering out irrelevant data and only letting valuable information propagate. Uses convolution to"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.2a: Spatial Filters: Convolutional Neural Networks\n",
    "\n",
    "A CNN applies a set of weights to input data, essentially, as a sliding dot product. Easiest to imagine convolutions in image processing, where _convolving_ an image with a gaussian filter results in a \"smoothed\" or blurred image.\n",
    "\n",
    "<img src=\"images/Screen Shot 2022-04-08 at 10.58.46 AM.png/\">\n",
    "\n",
    "Instead of defining a gaussian filter, CNNs can initialize random weights and, through training, learn the weights that improve accuracy.\n",
    "\n",
    "filters can be applied as a sliding window. The dimensionality of the output will be reduced to (number of horizontal window moves) X (number of vertical window moves)\n",
    "\n",
    "**max pooling**:\n",
    "A filter that returns the largest value (within the filter window) of the input it is applied to.\n",
    "\n",
    "\n",
    "**CNNs for text**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. define the maximum number of words to keep, based on word frquency. only the most common <num_words> - 1 will be kept.\n",
    "2. Create the vocabulary. aka. word_index dictionary\n",
    "3. `.text_to_sequences`: converts each text to a sequence of integers found in the vocabulary dictionary\n",
    "4. `pad_sequences`: pads the sequences from step 3 with zeros so they are all the same length\n",
    "5. `pd.get_dummies`: converts a binary array into dummy codes, e.g from [1, 1, 0] --> [(0,1), (0, 1), (1, 0)]\n",
    "6. tain_test_split:  Split arrays or matrices into random train and test subsets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "tokenizer = Tokenizer(num_words=max_words, split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X)\n",
    "Y = pd.get_dummies(data['label']).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, random_state = 36)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**creating an embedding layer**\n",
    "These can be thought of an alternative to one-hot encoding along with dimensionality reduction. Will be discussed in chapter 3.\n",
    "\n",
    "* The model below contains 3 convolutional layers. Each of which specified the dimensionality of the output and the size of filter (kernel size).\n",
    "* The flatten layer coerces the 65x16 output of the final convolutional layer into a 1040-dimensional array, which is fed to the dropout layer.\n",
    "* The dropout layer randomly resets a specified fraction of its input unites to 0 during training. this prevents overfitting.\n",
    "* The Dense layer contains the binary representation of the output labels\n",
    "\n",
    "<img src=\"images/Screen Shot 2022-04-08 at 11.45.26 AM.png \">\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 47, 100)           100000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 47, 64)            19264     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 47, 32)            6176      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 47, 16)            1552      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 752)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 752)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 1506      \n",
      "=================================================================\n",
      "Total params: 128,498\n",
      "Trainable params: 128,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_words, embedding_vector_length, input_length=X.shape[1]))\n",
    "model.add(Convolution1D(64, 3, padding=\"same\"))\n",
    "model.add(Convolution1D(32, 3, padding=\"same\"))\n",
    "model.add(Convolution1D(16, 3, padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "160/160 [==============================] - 1s 8ms/step - loss: 0.6938 - accuracy: 0.5094\n",
      "Epoch 2/3\n",
      "160/160 [==============================] - 0s 598us/step - loss: 0.6654 - accuracy: 0.6562\n",
      "Epoch 3/3\n",
      "160/160 [==============================] - 0s 697us/step - loss: 0.6449 - accuracy: 0.7000\n",
      "Accuracy: 45.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "# Evaluation on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Deep Learning and NLP: A new paradigm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f7c24000",
   "language": "python",
   "display_name": "PyCharm (mTurkBx)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}