{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chapter 2: Deep Learning and Language: The Basics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1: Basic Architectures of Deep Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.1: Deep Multilayer Perceptrons\n",
    "\n",
    "artificial  neurons are mathematical functions that receive weighted input from their afferents."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Convolution1D, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. `.fit_on_text` : creates a vocabulary index based on word frequency. mapping a word to it's index within text\n",
    "    * e.g. given `\"the cat sat on the mat\"` , then it will create a dictionary such that `word_index['sat']` will return `2`.\n",
    "    * The most frequent words are earlier indices so, `word_index[\"the\"]` will return `0`\n",
    "    * Often, the first few indices will be stop words because they appear a lot.\n",
    "2. `.text_to_matrix` : creates a numpy matrix with rows corresponding to the number of documents and columns corresponding to the unique words in the vocabulary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = pd.read_csv('../raaijmakers-master-code/code/pos_neg.txt', sep='\\t',  encoding = \"ISO-8859-1\")\n",
    "docs = data['text']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "X_train = tokenizer.texts_to_matrix(docs, mode='binary')\n",
    "Y_train = np_utils.to_categorical(data['label'])  # from [1, 1, 0] --> [(0,1), (0, 1), (1, 0)]\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "nb_classes = Y_train.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "856"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.word_index['bad']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Building the model\n",
    "1. The Sequential model serves as a container for the stacked layers.\n",
    "2. Add a dense layer to receive inputs (of dimensionality defined above) and return an output of 128 dimensions\n",
    "3. Add another layer that receives, as inputs, the outputs of the Dense layer in 2 and uses a ReLU activation function to determine its output."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=input_dim))\n",
    "model.add(Activation('relu'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding more layers...."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once you are finished adding layers, `compile` by specifying:\n",
    " * the loss(error) function\n",
    " * a numerical optimizer algorithm to carry out the gradient descent process\n",
    " * a metric to evaluate performance.\n",
    "\n",
    "Next, you can fit the model specifying\n",
    " * validation_split: the proportion of data held out of training and used for testing\n",
    " * the number of training epochs\n",
    " * batch size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.6926 - accuracy: 0.4889 - val_loss: 0.7528 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6563 - accuracy: 0.5556 - val_loss: 0.8085 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.5394 - accuracy: 0.8389 - val_loss: 0.7981 - val_accuracy: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.2616 - accuracy: 0.9722 - val_loss: 0.8000 - val_accuracy: 0.3500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.3560 - val_accuracy: 0.8500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1179 - val_accuracy: 0.9000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 7.2622e-05 - accuracy: 1.0000 - val_loss: 0.0600 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 8.0652e-06 - accuracy: 1.0000 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.8716e-06 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 8.3067e-07 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7fa627a15d50>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.1, shuffle=True,verbose=2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.2: Spatial and Temporal Operators\n",
    "\n",
    "**Spatial Filtering: Convolutional Neural Networks**\n",
    "The goal of spatial filtering is to deal with the _structure_ of input data by filtering out irrelevant data and only letting valuable information propagate. Uses convolution to"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.2a: Spatial Filters: Convolutional Neural Networks\n",
    "\n",
    "A CNN applies a set of weights to input data, essentially, as a sliding dot product. Easiest to imagine convolutions in image processing, where _convolving_ an image with a gaussian filter results in a \"smoothed\" or blurred image.\n",
    "\n",
    "<img src=\"images/Screen Shot 2022-04-08 at 10.58.46 AM.png/\">\n",
    "\n",
    "Instead of defining a gaussian filter, CNNs can initialize random weights and, through training, learn the weights that improve accuracy.\n",
    "\n",
    "filters can be applied as a sliding window. The dimensionality of the output will be reduced to (number of horizontal window moves) X (number of vertical window moves)\n",
    "\n",
    "**max pooling**:\n",
    "A filter that returns the largest value (within the filter window) of the input it is applied to.\n",
    "\n",
    "\n",
    "**CNNs for text**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. define the maximum number of words to keep, based on word frquency. only the most common <num_words> - 1 will be kept.\n",
    "2. Create the vocabulary. aka. word_index dictionary\n",
    "3. `.text_to_sequences`: converts each text to a sequence of integers found in the vocabulary dictionary\n",
    "4. `pad_sequences`: pads the sequences from step 3 with zeros so they are all the same length\n",
    "5. `pd.get_dummies`: converts a binary array into dummy codes, e.g from [1, 1, 0] --> [(0,1), (0, 1), (1, 0)]\n",
    "6. tain_test_split:  Split arrays or matrices into random train and test subsets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "tokenizer = Tokenizer(num_words=max_words, split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X)\n",
    "Y = pd.get_dummies(data['label']).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, random_state = 36)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**creating an embedding layer**\n",
    "These can be thought of an alternative to one-hot encoding along with dimensionality reduction. Will be discussed in chapter 3.\n",
    "\n",
    "* The model below contains 3 convolutional layers. Each of which specified the dimensionality of the output and the size of filter (kernel size).\n",
    "* The flatten layer coerces the 65x16 output of the final convolutional layer into a 1040-dimensional array, which is fed to the dropout layer.\n",
    "* The dropout layer randomly resets a specified fraction of its input unites to 0 during training. this prevents overfitting.\n",
    "* The Dense layer contains the binary representation of the output labels\n",
    "\n",
    "<img src=\"images/Screen Shot 2022-04-08 at 11.45.26 AM.png \">\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 47, 100)           100000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 47, 64)            19264     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 47, 32)            6176      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 47, 16)            1552      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 752)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 752)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 1506      \n",
      "=================================================================\n",
      "Total params: 128,498\n",
      "Trainable params: 128,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 100\n",
    "\n",
    "model = Sequential()\n",
    "Embedding()\n",
    "model.add(Embedding( max_words, embedding_vector_length, input_length=X.shape[1]))\n",
    "model.add(Convolution1D(64, 3, padding=\"same\"))\n",
    "model.add(Convolution1D(32, 3, padding=\"same\"))\n",
    "model.add(Convolution1D(16, 3, padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.6133 - accuracy: 0.7219\n",
      "Epoch 2/6\n",
      "160/160 [==============================] - 0s 629us/step - loss: 0.5765 - accuracy: 0.7688\n",
      "Epoch 3/6\n",
      "160/160 [==============================] - 0s 647us/step - loss: 0.5306 - accuracy: 0.7656\n",
      "Epoch 4/6\n",
      "160/160 [==============================] - 0s 685us/step - loss: 0.4997 - accuracy: 0.7750\n",
      "Epoch 5/6\n",
      "160/160 [==============================] - 0s 626us/step - loss: 0.4663 - accuracy: 0.7812\n",
      "Epoch 6/6\n",
      "160/160 [==============================] - 0s 646us/step - loss: 0.4168 - accuracy: 0.8656\n",
      "Accuracy: 58.75%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=6, batch_size=64)\n",
    "\n",
    "# Evaluation on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.2b: Temporal Filtering: Recurrent Neural Networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/rnn.png \">\n",
    "\n",
    "Retropropagation can fix weights and biases, but earlier memory states can be improved by unrolling a cell multiple times.\n",
    "\n",
    "<img src=\"images/rnn_unrolled.png\">\n",
    "We consider the whole unrolled cell updates as one training step. If the final output (Y5) does not match the training label, the cell's states can be changed retroactively. Note that this is all the same cell  (w the same weights and biases)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* The code below begins with a list of words (just one) as data.\n",
    "* The set of unique characters in the data (6 of them) comprises the alphabet, which the label encoder 'fits' such that the unique letters are arranged  alphabetically as labels.\n",
    "\n",
    "* Next, the label encoder uses the `.fit_tranform` method on the alphabet to assign integers to each label, just a wrapper around `numpy.unique(ar=alphabet, return_inverse=True)`\n",
    "* These integers are then dummy coded to make one-hot vectors.\n",
    "\n",
    "_Training Data_\n",
    "Training data consists of X and Y, where X is the first 8 (out of a total 9) characters of the words in the data, and Y are the last 8.\n",
    "e.g. $$ X_1= \"c\", Y_1=\"h\", \\\\\n",
    "X_2=\"h\", Y_2=\"a\"...$$\n",
    "\n",
    "_Test Data_\n",
    "Same as Training\n",
    "\n",
    "After defining sample size and sample length, the code then bootstraps the training data by simply repeating it 256 times (the sample size).\n",
    "Training XY go from a list of 8 arrays with 6-elements each to a 256 x 8 x 6 tensor (sample size X length of training data X # unique characters in alphabet).\n",
    "    - in other words 256 identical 8x6 matrices\n",
    "\n",
    "Test data are also reshaped from a list of 8 arrays with 6-elements each, to a 1x8x6 tensor.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, TimeDistributed, Dense\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Basically onehot vectors of each letter in the data. Because there are 6 unique letters, vectors will be length 6\n",
    "data = ['character']\n",
    "alphabet = np.array(list(set([c for w in data for c in w]))) #an array of the unique letters in the data\n",
    "enc = LabelEncoder()\n",
    "enc.fit(alphabet) # once the encoder is fit, it has a classes_ attribute containing unique labels\n",
    "int_enc = enc.fit_transform(alphabet) #assigns an integer to each label in the encoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "int_enc = int_enc.reshape(len(int_enc), 1) # from [int, int, int] to -> [[int], [int], [int]]\n",
    "onehot_encoded = onehot_encoder.fit_transform(int_enc) # transforms the array of integer encodings into dummy codes.\n",
    "# every unique letter in the data comprises the alphabet (which will become the labels).\n",
    "# The label encoder assigns an integer to each letter in the alphabet (int_enc)\n",
    "# the one-hot encoder transforms integers to one-hot vectors.\n",
    "# letter -> integer -> onehot vecotr\n",
    "\n",
    "\n",
    "# since the data consists of one word.\n",
    "# X_train is are the first n-1 letters in the word (as one hot vectors).\n",
    "# For each letter in X_train, Y_train, corresponds to the letter that follows it.\n",
    "#X_train = [[c],[h],[a],[r],[a],[c],[t],[e]]\n",
    "#Y_train = [[h],[a],[r],[a],[c],[t],[e],[r]]\n",
    "#...the letters above are represented as one-hot vectors.\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for w in data:\n",
    "    for i in range(len(w)-1):\n",
    "        X_train.extend(onehot_encoder.transform([enc.transform([w[i]])]))\n",
    "        Y_train.extend(onehot_encoder.transform([enc.transform([w[i+1]])]))\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "test_data = ['character']\n",
    "\n",
    "for w in test_data:\n",
    "    for i in range(len(w)-1):\n",
    "        X_test.extend(onehot_encoder.transform([enc.transform([w[i]])]))\n",
    "        Y_test.extend(onehot_encoder.transform([enc.transform([w[i+1]])]))\n",
    "\n",
    "sample_size = 256\n",
    "sample_len = len(X_train)\n",
    "\n",
    "# Training XY go from a 8-list of 6-element arrays to a (256sample_size x 8(length fo training data), x 6 (# of unique characters in alphabet)) tensor\n",
    "X_train = np.array([X_train * sample_size]).reshape(sample_size, sample_len, len(alphabet))\n",
    "Y_train = np.array([Y_train * sample_size]).reshape(sample_size, sample_len, len(alphabet))\n",
    "\n",
    "\n",
    "test_len = len(X_test)\n",
    "X_test = np.array([X_test]).reshape(1, test_len, len(alphabet))\n",
    "Y_test = np.array([Y_test]).reshape(1, test_len, len(alphabet))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0.],\n       [1., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1.],\n       [0., 0., 1., 0., 0., 0.]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First column corresponds to the letter \"a\", there is a 1.0 in rows 2 and 4 because those are the indices of\n",
    "#[[c],[h],[a],[r],[a],[c],[t],[e]] containing an \"a\".\n",
    "X_train[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Building the RNN\n",
    "\n",
    "* Starts by initializing an instance of the Sequential model class\n",
    "* Add a SimpleRNN layer defining input dimensionality to be alphabet length (since input will be each of the 8 rows of 6 elements)\n",
    "* Add a Densely connected layer that is where weights are applied horizontally at every temporal slice of input.\n",
    "* Compile the model defining loss function and optimizer, then fit on the training data and predict the labels of the test data.\n",
    "* The predictions are probability values for each letter in each subsequent time point, i.e.:\n",
    "    - $p(character_{i+1})={a:.3,r:.5,e:.6,...}$\n",
    "    - $p(character_{i+2})={a:.8,r:.3,e:.2,...}$\n",
    "* We take the max of these predictions and use `enc.inverse_transform` for go from integer back to string character\n",
    "* Finally we print the value of the loss function (crossentropy) and evaluation metric (accuracy) for the model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \n",
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(return_sequences=True, input_shape=(None, 6), units=100)`\n",
      "  \n",
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=6)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "256/256 [==============================] - 1s 5ms/step - loss: 0.5612 - accuracy: 0.7812\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 0s 279us/step - loss: 0.3690 - accuracy: 0.9635\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 0s 287us/step - loss: 0.2640 - accuracy: 0.9844\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 0s 297us/step - loss: 0.1984 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 0s 288us/step - loss: 0.1557 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 0s 302us/step - loss: 0.1275 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 0s 301us/step - loss: 0.1078 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 0s 297us/step - loss: 0.0932 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 0s 430us/step - loss: 0.0818 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 0s 308us/step - loss: 0.0727 - accuracy: 1.0000\n",
      "h\n",
      "a\n",
      "r\n",
      "a\n",
      "c\n",
      "t\n",
      "e\n",
      "r\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "[0.06813694536685944, 1.0]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(input_dim=len(alphabet), output_dim=100, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(output_dim=len(alphabet), activation=\"sigmoid\")))\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model.fit(X_train, Y_train, nb_epoch=10, batch_size=32)\n",
    "\n",
    "preds = model.predict(X_test)[0]\n",
    "for p in preds:\n",
    "    m=np.argmax(p)\n",
    "    print(enc.inverse_transform([m])[0])\n",
    "print(model.evaluate(X_test, Y_test, batch_size=32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.35140693, 0.3050174 , 0.24562523, 0.59697104, 0.27344728,\n        0.25665528],\n       [0.8689045 , 0.05291989, 0.05919242, 0.03941149, 0.03320614,\n        0.03217295],\n       [0.03738743, 0.01297069, 0.02276149, 0.01652685, 0.9423271 ,\n        0.01211572],\n       [0.9748498 , 0.01938614, 0.00741681, 0.00337327, 0.01314285,\n        0.0118539 ],\n       [0.02507344, 0.97005314, 0.01903486, 0.02007443, 0.02824026,\n        0.01363108],\n       [0.01787403, 0.02216244, 0.00761157, 0.01090309, 0.01122886,\n        0.96163845],\n       [0.02933225, 0.01139101, 0.9580256 , 0.01354322, 0.01815313,\n        0.00988832],\n       [0.01280633, 0.00583234, 0.01528642, 0.01604271, 0.97839177,\n        0.00858387]], dtype=float32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row is a subsequent prediction. Each column represents an integer that represents a character in the alphabet\n",
    "preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While this simple RNN does will for this tiny sequence of letters, they fail on longer sequences and blindly re-use hidden states in their entirety.\n",
    "\n",
    "**Long Short Term Memory Networks**\n",
    "Gates the information passed from the past into the present.\n",
    "LSTM cells encode contextual information - : they make this information available on local positions in the time-distributed case, and globally (e.g. for an entire sequence of words) in the non-time-distributed case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x [[0. 0. 0. 0. 1. 0. 0.]]\n",
      "1 y [[0. 0. 0. 0. 0. 1. 0.]]\n",
      "2 z [[0. 0. 0. 0. 0. 0. 1.]]\n",
      "3 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "4 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "5 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "6 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "7 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "8 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "9 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "10 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "11 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "12 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "13 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "14 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "15 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "16 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "17 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "18 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "19 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "20 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "21 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "22 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "23 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "24 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "25 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "26 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "27 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "28 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "29 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "30 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "31 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "32 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "33 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "34 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "35 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "36 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "37 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "38 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "39 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "40 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "41 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "42 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "43 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "44 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "45 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "46 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "47 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "48 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "49 x [[0. 0. 0. 0. 1. 0. 0.]]\n",
      "50 y [[0. 0. 0. 0. 0. 1. 0.]]\n",
      "0 p [[0. 1. 0. 0. 0. 0. 0.]]\n",
      "1 q [[0. 0. 1. 0. 0. 0. 0.]]\n",
      "2 r [[0. 0. 0. 1. 0. 0. 0.]]\n",
      "3 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "4 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "5 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "6 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "7 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "8 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "9 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "10 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "11 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "12 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "13 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "14 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "15 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "16 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "17 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "18 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "19 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "20 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "21 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "22 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "23 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "24 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "25 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "26 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "27 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "28 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "29 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "30 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "31 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "32 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "33 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "34 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "35 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "36 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "37 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "38 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "39 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "40 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "41 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "42 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "43 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "44 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "45 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "46 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "47 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "48 a [[1. 0. 0. 0. 0. 0. 0.]]\n",
      "49 p [[0. 1. 0. 0. 0. 0. 0.]]\n",
      "50 q [[0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/ipykernel_launcher.py:51: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/ipykernel_launcher.py:51: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=True, input_shape=(None, 7), units=100)`\n",
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/ipykernel_launcher.py:52: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=7)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 593ms/step\n",
      "[Iteration 1] score=0.317927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/ipykernel_launcher.py:63: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.5085 - accuracy: 0.8599\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[Iteration 2] score=0.973389\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryangonzalez/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/ipykernel_launcher.py:63: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 4s 7ms/step - loss: 0.1548 - accuracy: 0.9721\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[Iteration 3] score=0.971989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 7ms/step - loss: 0.1253 - accuracy: 0.9720\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[Iteration 4] score=0.971989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 7ms/step - loss: 0.1198 - accuracy: 0.9720\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[Iteration 5] score=0.971989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 7ms/step - loss: 0.1167 - accuracy: 0.9720\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[Iteration 6] score=0.971989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 8ms/step - loss: 0.1144 - accuracy: 0.9720\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[Iteration 7] score=0.971989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 8ms/step - loss: 0.1123 - accuracy: 0.9720\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[Iteration 8] score=0.971989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 8ms/step - loss: 0.1103 - accuracy: 0.9720\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[Iteration 9] score=0.971989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 4s 9ms/step - loss: 0.1083 - accuracy: 0.9720\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "[Iteration 10] score=0.971989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.1061 - accuracy: 0.9720\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[Iteration 11] score=0.971989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.1035 - accuracy: 0.9720\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[Iteration 12] score=0.971989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 7s 13ms/step - loss: 0.1003 - accuracy: 0.9720\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[Iteration 13] score=0.971989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0962 - accuracy: 0.9720\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[Iteration 14] score=0.971989\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 13ms/step - loss: 0.0908 - accuracy: 0.9731\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "[Iteration 15] score=0.974790\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 0.0856 - accuracy: 0.9759\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "[Iteration 16] score=0.974790\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0779 - accuracy: 0.9774\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "[Iteration 17] score=0.976190\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0771 - accuracy: 0.9784\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "[Iteration 18] score=0.981793\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 0.0652 - accuracy: 0.9809\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[Iteration 19] score=0.978992\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 11ms/step - loss: 0.0589 - accuracy: 0.9815\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[Iteration 20] score=0.981793\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0545 - accuracy: 0.9820\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[Iteration 21] score=0.983193\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0512 - accuracy: 0.9832\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[Iteration 22] score=0.983193\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 7s 13ms/step - loss: 0.0480 - accuracy: 0.9836\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "[Iteration 23] score=0.983193\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 0.0450 - accuracy: 0.9842\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[Iteration 24] score=0.983193\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0436 - accuracy: 0.9839\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "[Iteration 25] score=0.983193\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0413 - accuracy: 0.9849\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "[Iteration 26] score=0.984594\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 13ms/step - loss: 0.0407 - accuracy: 0.9842\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[Iteration 27] score=0.985994\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0382 - accuracy: 0.9856\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[Iteration 28] score=0.985994\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 11ms/step - loss: 0.0373 - accuracy: 0.9855\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "[Iteration 29] score=0.985994\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0363 - accuracy: 0.9871\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[Iteration 30] score=0.985994\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 11ms/step - loss: 0.0344 - accuracy: 0.9884\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "[Iteration 31] score=0.988796\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0336 - accuracy: 0.9890\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "[Iteration 32] score=0.987395\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 0.0323 - accuracy: 0.9894\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "[Iteration 33] score=0.990196\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 8s 15ms/step - loss: 0.0312 - accuracy: 0.9898\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "[Iteration 34] score=0.987395\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 11ms/step - loss: 0.0306 - accuracy: 0.9899\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[Iteration 35] score=0.990196\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0291 - accuracy: 0.9914\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "[Iteration 36] score=0.992997\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 7s 14ms/step - loss: 0.0283 - accuracy: 0.9928\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[Iteration 37] score=0.991597\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0275 - accuracy: 0.9943\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[Iteration 38] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0265 - accuracy: 0.9952\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[Iteration 39] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 0.0256 - accuracy: 0.9948\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[Iteration 40] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0247 - accuracy: 0.9954\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[Iteration 41] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0236 - accuracy: 0.9956\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "[Iteration 42] score=0.994398\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0231 - accuracy: 0.9959\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "[Iteration 43] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 9ms/step - loss: 0.0222 - accuracy: 0.9964\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[Iteration 44] score=0.994398\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 9s 18ms/step - loss: 0.0220 - accuracy: 0.9957\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "[Iteration 45] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 0.0208 - accuracy: 0.9967\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "[Iteration 46] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0200 - accuracy: 0.9966\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "[Iteration 47] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 0.0198 - accuracy: 0.9962\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[Iteration 48] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 0.0185 - accuracy: 0.9969\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "[Iteration 49] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 13ms/step - loss: 0.0188 - accuracy: 0.9963\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[Iteration 50] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 0.0178 - accuracy: 0.9967\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[Iteration 51] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0173 - accuracy: 0.9963\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "[Iteration 52] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 11s 22ms/step - loss: 0.0167 - accuracy: 0.9964\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "[Iteration 53] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0164 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[Iteration 54] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0156 - accuracy: 0.9966\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "[Iteration 55] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 0.0153 - accuracy: 0.9966\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "[Iteration 56] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0149 - accuracy: 0.9966\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "[Iteration 57] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 0.0149 - accuracy: 0.9966\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[Iteration 58] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0144 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "[Iteration 59] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 8s 16ms/step - loss: 0.0138 - accuracy: 0.9966\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "[Iteration 60] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 8s 15ms/step - loss: 0.0132 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[Iteration 61] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0134 - accuracy: 0.9964\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[Iteration 62] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0129 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[Iteration 63] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0123 - accuracy: 0.9967\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "[Iteration 64] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 11s 22ms/step - loss: 0.0118 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "[Iteration 65] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 7s 13ms/step - loss: 0.0123 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "[Iteration 66] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 7s 13ms/step - loss: 0.0117 - accuracy: 0.9966\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "[Iteration 67] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 9s 17ms/step - loss: 0.0111 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[Iteration 68] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 9s 17ms/step - loss: 0.0104 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[Iteration 69] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 8s 15ms/step - loss: 0.0116 - accuracy: 0.9967\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[Iteration 70] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 0.0108 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[Iteration 71] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 7s 14ms/step - loss: 0.0108 - accuracy: 0.9967\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[Iteration 72] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 9ms/step - loss: 0.0096 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[Iteration 73] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0093 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "[Iteration 74] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0109 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 320ms/step\n",
      "[Iteration 75] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 10s 19ms/step - loss: 0.0115 - accuracy: 0.9963\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "[Iteration 76] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0093 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 349ms/step\n",
      "[Iteration 77] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 13s 25ms/step - loss: 0.0100 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "[Iteration 78] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0087 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[Iteration 79] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 8s 16ms/step - loss: 0.0084 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "[Iteration 80] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 7s 13ms/step - loss: 0.0082 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[Iteration 81] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 9ms/step - loss: 0.0081 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "[Iteration 82] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 8s 15ms/step - loss: 0.0108 - accuracy: 0.9962\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[Iteration 83] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 7s 14ms/step - loss: 0.0084 - accuracy: 0.9970\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "[Iteration 84] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0078 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[Iteration 85] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 11ms/step - loss: 0.0076 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "[Iteration 86] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 8s 15ms/step - loss: 0.0092 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[Iteration 87] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 9s 18ms/step - loss: 0.0089 - accuracy: 0.9965\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[Iteration 88] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0081 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "[Iteration 89] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 17s 33ms/step - loss: 0.0072 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "[Iteration 90] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 14s 28ms/step - loss: 0.0071 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "[Iteration 91] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0070 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "[Iteration 92] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0099 - accuracy: 0.9964\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[Iteration 93] score=0.991597\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0118 - accuracy: 0.9964\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[Iteration 94] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0097 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[Iteration 95] score=0.995798\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0084 - accuracy: 0.9966\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[Iteration 96] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0096 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "[Iteration 97] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 9ms/step - loss: 0.0081 - accuracy: 0.9966\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[Iteration 98] score=0.998599\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 12ms/step - loss: 0.0093 - accuracy: 0.9966\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[Iteration 99] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 11ms/step - loss: 0.0082 - accuracy: 0.9969\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "[Iteration 100] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0080 - accuracy: 0.9968\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[Iteration 101] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0067 - accuracy: 0.9974\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "[Iteration 102] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 7s 13ms/step - loss: 0.0063 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[Iteration 103] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0062 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "[Iteration 104] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0061 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "[Iteration 105] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 7s 15ms/step - loss: 0.0060 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "[Iteration 106] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 8s 15ms/step - loss: 0.0059 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[Iteration 107] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 9ms/step - loss: 0.0058 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[Iteration 108] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 11s 22ms/step - loss: 0.0145 - accuracy: 0.9949\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "[Iteration 109] score=0.994398\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 9s 18ms/step - loss: 0.0143 - accuracy: 0.9949\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[Iteration 110] score=0.994398\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0098 - accuracy: 0.9963\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "[Iteration 111] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 6s 11ms/step - loss: 0.0074 - accuracy: 0.9975\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "[Iteration 112] score=0.997199\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 5s 10ms/step - loss: 0.0068 - accuracy: 0.9972\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "[Iteration 113] score=0.997199\n",
      "Epoch 1/1\n",
      "384/512 [=====================>........] - ETA: 1s - loss: 0.0065 - accuracy: 0.9972"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-3627fb6c3d26>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0mn\u001B[0m\u001B[0;34m+=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m         \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnb_epoch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[0mpreds\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001B[0m\n\u001B[1;32m   1237\u001B[0m                                         \u001B[0msteps_per_epoch\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msteps_per_epoch\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1238\u001B[0m                                         \u001B[0mvalidation_steps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvalidation_steps\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1239\u001B[0;31m                                         validation_freq=validation_freq)\n\u001B[0m\u001B[1;32m   1240\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1241\u001B[0m     def evaluate(self,\n",
      "\u001B[0;32m~/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001B[0m in \u001B[0;36mfit_loop\u001B[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001B[0m\n\u001B[1;32m    194\u001B[0m                     \u001B[0mins_batch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mins_batch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m                 \u001B[0mouts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfit_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mins_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m                 \u001B[0mouts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mto_list\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mouts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0ml\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mo\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout_labels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mouts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m   3738\u001B[0m         \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmath_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3739\u001B[0m       \u001B[0mconverted_inputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3740\u001B[0;31m     \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_graph_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mconverted_inputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3741\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3742\u001B[0m     \u001B[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1079\u001B[0m       \u001B[0mTypeError\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mFor\u001B[0m \u001B[0minvalid\u001B[0m \u001B[0mpositional\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mkeyword\u001B[0m \u001B[0margument\u001B[0m \u001B[0mcombinations\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1080\u001B[0m     \"\"\"\n\u001B[0;32m-> 1081\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1082\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1083\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcancellation_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, args, kwargs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1119\u001B[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001B[1;32m   1120\u001B[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001B[0;32m-> 1121\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_flat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcaptured_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcancellation_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1122\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1123\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_filtered_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1222\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mexecuting_eagerly\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1223\u001B[0m       flat_outputs = forward_function.call(\n\u001B[0;32m-> 1224\u001B[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001B[0m\u001B[1;32m   1225\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1226\u001B[0m       \u001B[0mgradient_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_delayed_rewrite_functions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mregister\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    509\u001B[0m               \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    510\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"executor_type\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexecutor_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"config_proto\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 511\u001B[0;31m               ctx=ctx)\n\u001B[0m\u001B[1;32m    512\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    513\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[0;32m~/opt/anaconda3/envs/mTurkBx/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     59\u001B[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001B[1;32m     60\u001B[0m                                                \u001B[0mop_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattrs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m                                                num_outputs)\n\u001B[0m\u001B[1;32m     62\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import  LSTM, TimeDistributed, Dense\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "data = ['xyzaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaxyz',\n",
    "       'pqraaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaapqr']\n",
    "\n",
    "test_data = ['xyzaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaxyz',\n",
    "            'pqraaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaapqr']\n",
    "\n",
    "enc = LabelEncoder()\n",
    "alphabet = np.array(list(set([c for w in data for c in w])))\n",
    "enc.fit(alphabet)\n",
    "int_enc = enc.fit_transform(alphabet)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "int_enc = int_enc.reshape(len(int_enc), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(int_enc) # dummy coding\n",
    "\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "\n",
    "for w in data:\n",
    "    for i in range(len(w)-1):\n",
    "        X_train.extend(onehot_encoder.transform([enc.transform([w[i]])]))\n",
    "        y_train.extend(onehot_encoder.transform([enc.transform([w[i+1]])]))\n",
    "\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "\n",
    "for w in test_data:\n",
    "    for i in range(len(w)-1):\n",
    "        X_test.extend(onehot_encoder.transform([enc.transform([w[i]])]))\n",
    "        print(i,w[i],onehot_encoder.transform([enc.transform([w[i]])]))\n",
    "        y_test.extend(onehot_encoder.transform([enc.transform([w[i+1]])]))\n",
    "\n",
    "sample_size=512\n",
    "sample_len=len(X_train)\n",
    "\n",
    "X_train = np.array([X_train*sample_size]).reshape(sample_size,sample_len,len(alphabet))\n",
    "y_train = np.array([y_train*sample_size]).reshape(sample_size,sample_len,len(alphabet))\n",
    "\n",
    "test_len=len(X_test)\n",
    "X_test= np.array([X_test]).reshape(1,test_len,len(alphabet))\n",
    "y_test= np.array([y_test]).reshape(1,test_len,len(alphabet))\n",
    "\n",
    "model=Sequential()\n",
    "model.add(LSTM(input_dim  = len(alphabet), output_dim = 100, return_sequences = True))\n",
    "model.add(TimeDistributed(Dense(output_dim = len(alphabet), activation =  \"sigmoid\")))\n",
    "model.compile(loss=\"binary_crossentropy\",metrics=[\"accuracy\"], optimizer = \"adam\")\n",
    "\n",
    "n=1\n",
    "\n",
    "while True:\n",
    "        score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "        print(\"[Iteration %d] score=%f\"%(n,score[1]))\n",
    "        if score[1] == 1.0:\n",
    "            break\n",
    "        n+=1\n",
    "        model.fit(X_train, y_train, nb_epoch = 1, batch_size = 32)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y']\n",
      "['z']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['x']\n",
      "['y']\n",
      "['z']\n",
      "['q']\n",
      "['r']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['a']\n",
      "['p']\n",
      "['q']\n",
      "['r']\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0.006394200958311558, 0.9971988797187805]\n"
     ]
    }
   ],
   "source": [
    "preds=model.predict(X_test)[0]\n",
    "for p in preds:\n",
    "    m=np.argmax(p)\n",
    "    print(enc.inverse_transform([m]))\n",
    "\n",
    "print(model.evaluate(X_test,y_test,batch_size=32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Deep Learning and NLP: A new paradigm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f7c24000",
   "language": "python",
   "display_name": "PyCharm (mTurkBx)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}